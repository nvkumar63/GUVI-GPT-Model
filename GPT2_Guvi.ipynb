{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_lKn7dtWU_S"
      },
      "source": [
        "Uninstall conflicting packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeeMrs49afpt",
        "outputId": "bf21e576-2d9c-41a6-c487-362e83be2b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHaxwYKAXsHe"
      },
      "outputs": [],
      "source": [
        "# Uninstall conflicting packages\n",
        "!pip3 uninstall pyarrow -y\n",
        "!pip3 uninstall requests -y\n",
        "!pip install datasets\n",
        "\n",
        "\n",
        "# Install specific versions\n",
        "!pip3 install pyarrow==14.0.1\n",
        "!pip3 install requests==2.31.0\n",
        "\n",
        "# Reinstall dependencies\n",
        "!pip3 install cudf-cu12\n",
        "!pip3 install ibis-framework\n",
        "\n",
        "# Verify installation\n",
        "import pyarrow\n",
        "import requests\n",
        "\n",
        "print(f\"pyarrow version: {pyarrow.__version__}\")\n",
        "print(f\"requests version: {requests.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGg1wrAiX_PZ"
      },
      "outputs": [],
      "source": [
        "!pip3 install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HveemAIIVz09"
      },
      "outputs": [],
      "source": [
        "# Install the transformers library if not already installed\n",
        "!pip install transformers\n",
        "\n",
        "# Import the necessary library\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Login using the token\n",
        "login(token=\"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\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFztW-qXWn6h"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBjk9ZNZabxr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "def preprocess_data(input_file, output_file, tokenizer_name=\"gpt2\"):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_name)\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for line in lines:\n",
        "            # Strip leading/trailing whitespace\n",
        "            line = line.strip()\n",
        "\n",
        "            # Additional preprocessing steps\n",
        "            # Example: Convert all text to lowercase\n",
        "            line = line.lower()\n",
        "\n",
        "            # Example: Replace specific characters or patterns\n",
        "            line = re.sub(r'\\W', ' ', line)  # Replace non-word characters with space\n",
        "\n",
        "            # Tokenize the line\n",
        "            token_ids = tokenizer.encode(line, add_special_tokens=False)\n",
        "            # Convert token IDs back to tokens\n",
        "            tokenized_line = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "            # Convert tokens to text and remove special tokens\n",
        "            processed_line = \" \".join(tokenized_line).replace('Ġ', '').replace('Ċ', '').replace('�', '').strip()\n",
        "            processed_line = re.sub(r'[^\\w\\s]', '', processed_line)\n",
        "            processed_line = re.sub(r'\\bgu vi\\b','guvi',processed_line)\n",
        "            # Remove extra spaces\n",
        "            processed_line = re.sub(r'\\s+', ' ', processed_line)\n",
        "\n",
        "            # Write the processed line to the output file\n",
        "            f.write(processed_line + \"\\n\")\n",
        "\n",
        "# Example usage:\n",
        "input_file = \"/content/Guvi_Dataset.txt\"  # Make sure this path is correct\n",
        "output_file = \"processed_company_data.txt\"\n",
        "preprocess_data(input_file, output_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMmTHHakXBYT"
      },
      "source": [
        "Fine Tuneing the Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade datasets"
      ],
      "metadata": {
        "id": "mUltM1ZGeAJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "wxXeajF9QQRa",
        "outputId": "6b544e42-dc8b-488c-b280-0e7b65361115"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "load_dataset() got an unexpected keyword argument 'data_files'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-389c514de773>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/path/to/processed_data.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Define training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-389c514de773>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(file_path, tokenizer, block_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Create dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     tokenized_dataset = dataset.map(\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: load_dataset() got an unexpected keyword argument 'data_files'"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Create dataset\n",
        "def load_dataset(file_path, tokenizer, block_size=128):\n",
        "    dataset = load_dataset('text', data_files={'train': file_path})\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda e: tokenizer(e['text'], truncation=True, padding='max_length', max_length=block_size),\n",
        "        batched=True\n",
        "    )\n",
        "    return tokenized_dataset['train']\n",
        "\n",
        "train_dataset = load_dataset(\"/path/to/processed_data.txt\", tokenizer)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=1000,  # Save checkpoints more frequently for safety\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=200,\n",
        ")\n",
        "\n",
        "# Initialize data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained(\"/content/drive/MyDrive/fine_tuned_model12345\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/fine_tuned_model12345\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTYp6mWoYKPI"
      },
      "source": [
        "Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma14KMYgQgrv",
        "outputId": "0df46d54-7fa4-4d63-f193-0238baa1d0f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text: Who is guvi ceo\n",
            "Generated Text 1:\n",
            "Who is guvi ceo the best online learning platform for beginners\n",
            "\n",
            "guvi offers a comprehensive online learning platform with a vast library of courses and resources it is a leader in online learning with a reputation for providing a comprehensive learning experience for its users\n",
            "\n",
            "guvi offers a comprehensive online learning platform with a vast library of courses and resources it is a leader in online learning with a reputation for providing a comprehensive learning experience for its users\n",
            "\n",
            "guvi offers a comprehensive online learning platform with a vast library of courses and resources it is a leader in online learning with a reputation for providing a comprehensive learning experience for its users\n",
            "\n",
            "guvi offers a comprehensive online learning platform with a vast library of courses and resources it is a leader in online learning with a reputation for providing a comprehensive learning experience for its users\n",
            "\n",
            "guvi offers a comprehensive online learning platform with a vast library of courses and resources it is a leader in online learning with a reputation for providing a comprehensive learning experience for its users\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#!pip install transformers\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_name_or_path = \"/content/drive/MyDrive/fine_tuned_model12345\"  # Use the directory where you saved the model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
        "\n",
        "token_name_or_path = \"/content/drive/MyDrive/fine_tuned_model12345\"  # Use the directory where you saved the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(token_name_or_path)\n",
        "\n",
        "# Set the pad_token to eos_token if it's not already set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define the text generation function\n",
        "def generate_text(model, tokenizer, seed_text, max_length=100, temperature=1.0, num_return_sequences=1):\n",
        "    # Tokenize the input text with padding\n",
        "    inputs = tokenizer(seed_text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    # Generate text\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.01,\n",
        "            pad_token_id=tokenizer.eos_token_id  # Ensure padding token is set to eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_texts = []\n",
        "    for i in range(num_return_sequences):\n",
        "        generated_text = tokenizer.decode(output[i], skip_special_tokens=True)\n",
        "        generated_texts.append(generated_text)\n",
        "\n",
        "    return generated_texts\n",
        "\n",
        "# Test the model\n",
        "seed_text = input(\"Enter the text: \")\n",
        "generated_texts = generate_text(model, tokenizer, seed_text, max_length=200, temperature=0.9, num_return_sequences=1)\n",
        "\n",
        "for i, text in enumerate(generated_texts):\n",
        "    print(f\"Generated Text {i + 1}:\\n{text}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}